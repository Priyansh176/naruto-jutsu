# üî• Naruto Jutsu Recognition System

An interactive computer vision system that recognizes Naruto hand signs (jutsus) in real-time using MediaPipe and OpenCV. Perform hand sign sequences to trigger visual and sound effects!

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-green.svg)
![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10+-orange.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## ‚ú® Features

- **Real-time Hand Tracking**: Detects up to 2 hands simultaneously with 21 landmarks per hand
- **Gesture Recognition**: Identifies 12 Naruto hand signs (Tiger, Ram, Snake, Dragon, Boar, Dog, Monkey, Rabbit, Ox, Bird, Horse, Rat)
- **Sequence Detection**: Recognizes ordered hand sign sequences to detect jutsus
- **Visual Effects**: Particle systems (fire, water, lightning, smoke, earth) and screen flash effects
- **Sound Effects**: Multi-backend sound system with per-jutsu audio feedback
- **Guided Mode**: Visual sequence progression with highlighting and instant detection
- **Performance**: Runs at ‚â•15 FPS with <300ms latency on standard webcams

## üéØ Supported Jutsus

| Jutsu | Sequence | Time Window |
|-------|----------|-------------|
| Fireball Jutsu | Snake ‚Üí Ram ‚Üí Tiger | 5s |
| Shadow Clone | Ram ‚Üí Snake ‚Üí Tiger | 4s |
| Water Dragon | Ox ‚Üí Monkey ‚Üí Dragon ‚Üí Rat ‚Üí Boar ‚Üí Bird | 6s |
| Earth Wall | Snake ‚Üí Ram ‚Üí Horse | 4s |
| Summoning | Boar ‚Üí Dog ‚Üí Bird ‚Üí Monkey ‚Üí Ram | 7s |
| Chidori | Ox ‚Üí Rabbit ‚Üí Monkey | 4s |

## üìã Requirements

### System Requirements
- Python 3.9 or higher
- Webcam
- Windows/Linux/macOS (tested on Windows)

### Python Dependencies
```txt
opencv-python>=4.8.0
mediapipe>=0.10.0
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0
pygame>=2.5.0 (optional, for better sound)
```

## üöÄ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/naruto-jutsu-recognition.git
cd naruto-jutsu-recognition

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run the Application

```bash
# Start in Phase 3 mode (sequence detection with effects)
python naruto_jutsu/src/main.py
```

### Controls
- **Q**: Quit
- **1**: Switch to Phase 1 (Hand tracking only)
- **2**: Switch to Phase 2 (Gesture recognition)
- **3**: Switch to Phase 3 (Sequence detection)
- **M**: Open jutsu selection menu

## üìö Project Structure

```
naruto-jutsu-recognition/
‚îú‚îÄ‚îÄ naruto_jutsu/
‚îÇ   ‚îú‚îÄ‚îÄ data/                    # Training data (CSV files)
‚îÇ   ‚îú‚îÄ‚îÄ effects/                 # Visual effect assets
‚îÇ   ‚îú‚îÄ‚îÄ images/                  # Gesture reference images
‚îÇ   ‚îú‚îÄ‚îÄ models/                  # Trained ML models
‚îÇ   ‚îú‚îÄ‚îÄ sounds/                  # Sound effect files
‚îÇ   ‚îú‚îÄ‚îÄ src/                     # Source code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py             # Main application entry point
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hand_tracker.py     # MediaPipe hand tracking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_extractor.py # Feature extraction (72 features)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gesture_classifier.py # Random Forest classifier
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sequence_detector.py # FSM sequence detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ effects_engine.py   # Sound and visual effects
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ capture_data.py     # Data collection tool
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_model.py      # Model training script
‚îÇ   ‚îú‚îÄ‚îÄ tests/                   # Unit and performance tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_feature_extractor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_sequence_detector.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_effects_engine.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_tests.py        # Test runner
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ performance_test.py  # Performance validation
‚îÇ   ‚îî‚îÄ‚îÄ jutsus.json             # Jutsu definitions
‚îú‚îÄ‚îÄ prd.md                       # Product requirements
‚îú‚îÄ‚îÄ todo.md                      # Implementation roadmap
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îî‚îÄ‚îÄ README.md                    # This file
```

## üéì Training Your Own Model

### 1. Collect Training Data

```bash
# Launch data collection tool
python naruto_jutsu/src/capture_data.py

# Follow on-screen instructions:
# - Select gesture to capture
# - Perform hand sign for reference image
# - Capture 200+ samples per gesture
# - Data saved to naruto_jutsu/data/
```

### 2. Train the Model

```bash
# Train Random Forest classifier
python naruto_jutsu/src/train_model.py

# Model saved to: naruto_jutsu/models/gesture_classifier.pkl
```

### 3. Test the Model

```bash
# Run in Phase 2 mode to test gesture recognition
python naruto_jutsu/src/main.py
# Press '2' to switch to Phase 2
```

## üß™ Testing

### Run Unit Tests

```bash
# Run all unit tests
python naruto_jutsu/tests/run_tests.py

# Tests include:
# - Feature extraction (33 single hand + 72 two-hand features)
# - Sequence detection (FSM, time windows, targeting)
# - Effects engine (sounds, particles, flash)
```

### Run Performance Tests

```bash
# Validate FPS and latency
python naruto_jutsu/tests/performance_test.py

# Options:
# --camera 0    # Camera ID
# --duration 30 # Test duration in seconds

# Target metrics:
# - FPS: >= 15 FPS
# - Latency: < 300ms
```

## üé® How It Works

### Architecture

```
Webcam Feed
    ‚Üì
MediaPipe Hand Tracking (21 landmarks √ó 2 hands)
    ‚Üì
Feature Extraction (72-dimensional vector)
    ‚Üì
Random Forest Classifier (12 gestures)
    ‚Üì
Sequence Detector (FSM with time windows)
    ‚Üì
Effects Engine (sound + particles + flash)
    ‚Üì
Display (OpenCV window with UI overlay)
```

### Feature Extraction

**Single Hand (33 features):**
- 20 normalized distances (tip-to-wrist for each finger)
- 8 angles between finger segments
- 5 finger states (open/closed boolean)

**Two Hands (72 features):**
- 33 features for left hand
- 33 features for right hand
- 6 inter-hand features (distance, relative position)

### Sequence Detection

- **FSM-based**: Finite State Machine tracks ordered gestures
- **Time Windows**: Each jutsu has a specific time limit
- **Confidence Filtering**: Only accepts gestures with >70% confidence
- **Instant Detection**: No hold time in guided mode
- **Sound Feedback**: Plays sound on each gesture step (with debounce)

### Effects System

- **Multi-backend Sound**: Auto-detects pygame ‚Üí playsound ‚Üí winsound
- **Particle Types**: Fire, water, lightning, smoke, earth
- **Screen Flash**: Color-coded flash effect on jutsu detection
- **Extended Duration**: Effects visible for 3 seconds, name for 10 seconds

## üîß Configuration

### Jutsu Definitions (`jutsus.json`)

```json
{
  "jutsus": [
    {
      "id": "fireball",
      "name": "Fire Style: Fireball Jutsu",
      "japanese": "Katon: G≈çkaky≈´ no Jutsu",
      "sequence": ["Snake", "Ram", "Tiger"],
      "time_window": 5.0,
      "description": "A large ball of flame.",
      "effects": {
        "sound": "fireball.wav",
        "animation": "fire_burst",
        "color": [255, 100, 0],
        "particle_type": "fire"
      }
    }
  ],
  "settings": {
    "confidence_threshold": 0.7,
    "gesture_hold_time": 0.5,
    "reset_on_invalid": true
  }
}
```

### Adding New Jutsus

1. Add entry to `jutsus.json` with sequence and effects
2. Add corresponding sound file to `naruto_jutsu/sounds/`
3. System will automatically detect new jutsu

## üìä Performance Metrics

Tested on: Windows 11, Intel i7, 16GB RAM, Built-in webcam (1280x720)

| Metric | Target | Achieved | Status |
|--------|--------|----------|--------|
| FPS (Hand Tracking) | ‚â•15 FPS | ~30 FPS | ‚úÖ |
| FPS (Full Pipeline) | ‚â•15 FPS | ~25 FPS | ‚úÖ |
| Latency (Recognition) | <300ms | ~50ms | ‚úÖ |
| Accuracy (Good Lighting) | ‚â•90% | ~92% | ‚úÖ |

## üé¨ Demo

### Phase 1: Hand Tracking
- Shows 21 landmarks per hand
- FPS counter
- Finger state detection

### Phase 2: Gesture Recognition
- Real-time gesture classification
- Confidence display
- Latency measurement

### Phase 3: Sequence Detection + Effects
- Visual sequence progression
- Guided mode with highlighting
- Particle effects and screen flash
- Sound feedback on each step
- Jutsu name display (10 seconds)

## üêõ Troubleshooting

### Camera Not Opening
```bash
# Try different camera ID
python naruto_jutsu/src/main.py --camera 1
```

### Low FPS
- Reduce resolution in main.py (default: 1280x720)
- Close other applications
- Ensure good lighting for faster detection

### Classifier Not Found
```bash
# Collect data and train model first
python naruto_jutsu/src/capture_data.py
python naruto_jutsu/src/train_model.py
```

### Sound Not Playing
- Install pygame for better sound: `pip install pygame`
- Check that .wav files exist in `naruto_jutsu/sounds/`
- System will fall back to winsound (Windows) if pygame unavailable

## üìù Development Phases

- ‚úÖ **Phase 1**: Hand tracking (MediaPipe, 21 landmarks, 2 hands)
- ‚úÖ **Phase 2**: Gesture recognition (Random Forest, 72 features, 12 gestures)
- ‚úÖ **Phase 3**: Sequence detection (FSM, guided mode, visual feedback)
- ‚úÖ **Phase 4**: Effects engine (sound, particles, screen flash)
- üîÑ **Phase 5**: Testing & documentation (unit tests, performance tests, README)

## ü§ù Contributing

Contributions welcome! Areas for improvement:

- **More Jutsu**: Add more hand sign sequences
- **Better Effects**: Enhanced particle systems and animations
- **Mobile Support**: Port to mobile platforms
- **Accuracy**: Improve gesture classification with deep learning
- **UI**: Add Tkinter/PyQt GUI for better UX

## üìÑ License

MIT License - see LICENSE file for details.

## üôè Acknowledgments

- **MediaPipe**: Google's ML pipeline for hand tracking
- **OpenCV**: Computer vision library
- **Naruto**: Masashi Kishimoto's manga/anime series

## üìß Contact

Questions? Issues? Feel free to open an issue or reach out!

---

**Made with ‚ù§Ô∏è for anime fans and computer vision enthusiasts**

üî• Believe it! üî•
